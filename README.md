<div align="center">

<img src="assets/Nankai_University_logo.svg" height="80px" alt="Nankai University" >
<img src="assets/JittorLogo_Final1220.svg" height="80px" alt="Jittor" >

# Jittor-DEA-Net

**DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention (IEEE TIP 2024)**

[![Jittor](https://img.shields.io/badge/Framework-Jittor-EA3323.svg)](https://cg.cs.tsinghua.edu.cn/jittor/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/ZR-1N/Jittor-DEA-Net?style=social)](https://github.com/ZR-1N/Jittor-DEA-Net)

[![Paper](https://img.shields.io/badge/arXiv-Paper-b31b1b.svg)](https://arxiv.org/abs/2301.04805)
[![Original Repo](https://img.shields.io/badge/Official-PyTorch_Repo-EE4C2C.svg)](https://github.com/cecret3350/DEA-Net)

[English](#-introduction) | [ç®€ä½“ä¸­æ–‡](#-é¡¹ç›®ç®€ä»‹)

</div>

---

## ğŸ“– Introduction (é¡¹ç›®ç®€ä»‹)

This repository is an unofficial implementation of **DEA-Net** based on the [Jittor (è®¡å›¾)](https://cg.cs.tsinghua.edu.cn/jittor/) deep learning framework. This project is part of the **"Sprouts Program" at Nankai University**.

æœ¬é¡¹ç›®æ˜¯ IEEE TIP 2024 è®ºæ–‡ **DEA-Net** çš„**éå®˜æ–¹ Jittor (è®¡å›¾)** ç‰ˆæœ¬å¤ç°ï¼Œå±äº **å—å¼€å¤§å­¦â€œæ–°èŠ½è®¡åˆ’â€** å­¦ä¹ æˆæœã€‚

DEA-Net proposes a novel detail-enhanced convolution (DEConv) and content-guided attention (CGA) mechanism to effectively restore haze-free images. By leveraging Jittor's **Just-In-Time (JIT) compilation** and **operator fusion**, this implementation achieves competitive training efficiency compared to the PyTorch version while maintaining algorithmic performance.

DEA-Net æå‡ºäº†ä¸€ç§ç»†èŠ‚å¢å¼ºå·ç§¯ï¼ˆDEConvï¼‰å’Œå†…å®¹å¼•å¯¼æ³¨æ„åŠ›ï¼ˆCGAï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤å»é›¾å›¾åƒã€‚å¾—ç›Šäº Jittor æ¡†æ¶çš„ **å³æ—¶ç¼–è¯‘ (JIT)** å’Œ **ç®—å­èåˆ** æŠ€æœ¯ï¼Œæœ¬é¡¹ç›®åœ¨ä¿æŒåŸè®ºæ–‡ç²¾åº¦çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒä¸æ¨ç†ã€‚

This implementation has been shared with the official repository for reproducibility and community reference.

### Overall Architecture
<div align="center">
  <img src="fig/architecture.png" alt="Overall Architecture" width="90%">
</div>

### Results

<img src="fig/results.png" alt="Results" style="zoom:20%;" />

---
**Training Log**ï¼šTaking the ITS dataset as an example, the changes in the Loss and PSNR curves after 10 epochs of training are shown in the figure (Jittor is aligned with Pytorch).

**è®­ç»ƒæ—¥å¿—**ï¼šä»¥ITSæ•°æ®é›†ä¸ºä¾‹ï¼Œå…¶è®­ç»ƒ10ä¸ªepochçš„Lossä¸PSNRæ›²çº¿å˜åŒ–å¦‚å›¾ï¼ˆJittorä¸Pytorchå¯¹é½ï¼‰

<img src="fig/curves.png" alt="Results" style="zoom:20%;" />

## ğŸ“° News

- **[2025-12-21]** ğŸš€ Initial release of Jittor-DEA-Net code and pre-trained weights for HAZE4K, ITS, and OTS datasets.
- **[2025-11-17]** ğŸ—ï¸ Project initialized under Nankai University "Sprouts Program".

---

## ğŸ“Š Model Zoo & Results Comparisons (æ¨¡å‹åº“ä¸ç»“æœå¯¹æ¯”)

We have provided a comparison among the Jittor implementation, the PyTorch implementation (partially trained), and the official PyTorch implementation (fully convergent training).

æˆ‘ä»¬æä¾›äº† Jittor å®ç°ã€Pytorch å®ç°ï¼ˆéƒ¨åˆ†è®­ç»ƒï¼‰å’Œå®˜æ–¹ PyTorch å®ç°ï¼ˆå®Œå…¨æ”¶æ•›è®­ç»ƒï¼‰ä¹‹é—´çš„æ¯”è¾ƒã€‚

**Note:**
* **RESIDE-ITS:** We provide two versions. The **10-epoch** version allows for a direct comparison with the PyTorch implementation at the same stage. The 100-epoch version achieves a comparable SSIM to the official 300-epoch model, which may indicate the efficiency of the implementation.
* **Other Datasets:** The weights for HAZE4K and RESIDE-OTS are currently from the initial training phase (e.g., 10-30 epochs) but already demonstrate competitive performance.

**æ³¨æ„ï¼š**
* **RESIDE-ITS:** æˆ‘ä»¬æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬ã€‚**10 Epoch** ç‰ˆæœ¬ç”¨äºä¸åŒé˜¶æ®µçš„ PyTorch å®ç°è¿›è¡Œç›´æ¥å¯¹æ¯”ã€‚100 epochç‰ˆæœ¬å®ç°äº†ä¸å®˜æ–¹300 epochæ¨¡å‹ç›¸å½“çš„SSIMï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬å¤ç°çš„ç²¾åº¦ã€‚
* **å…¶ä»–æ•°æ®é›†:** HAZE4K å’Œ RESIDE-OTS çš„æƒé‡ç›®å‰ä»å¤„äºè®­ç»ƒåˆæœŸï¼ˆçº¦ 10-30 Epochï¼‰ï¼Œä½†å·²å±•ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚

| Dataset | Framework | Epochs Trained | PSNR (dB) | SSIM | Download Link |
| :--- | :--- | :---: | :---: | :---: | :---: |
| **HAZE4K** | **Jittor (Ours)** | **30** (Partial) | 32.54 | 0.9848 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | PyTorch (Ours) | 10 | 31.17 | 0.9813 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1QI3yJEXezM30hhdjgW_DZbaIspwCGhIO?usp=sharing) |
| | PyTorch (Official) | 300 | 34.26 | 0.9985 | [Google Drive](https://drive.google.com/drive/folders/1Rjb8dpyNnvvr0XLvIX9fg8Hdru_MhMCj?usp=sharing) / [Baidu (pwd:dcyb)](https://pan.baidu.com/s/1retfKIs_Om-D4zA45sL6Kg?pwd=dcyb) |
| **RESIDE-ITS** | **Jittor (Ours)** | **10** (Partial) | 35.87 | 0.9893 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | **Jittor (Ours)** | **100**(Partial) | **40.41** | **0.9956** | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | PyTorch (Ours) | 10 | 35.78 | 0.9876 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1QI3yJEXezM30hhdjgW_DZbaIspwCGhIO?usp=sharing) |
| | PyTorch (Official) | 300 | 41.31 | 0.9945 | [Google Drive](https://drive.google.com/drive/folders/1Rjb8dpyNnvvr0XLvIX9fg8Hdru_MhMCj?usp=sharing) / [Baidu (pwd:dcyb)](https://pan.baidu.com/s/1retfKIs_Om-D4zA45sL6Kg?pwd=dcyb) |
| **RESIDE-OTS** | **Jittor (Ours)** | **10** (Partial) | 32.71 | 0.9840 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | PyTorch (Official) | 300 | 36.59 | 0.9897 | [Google Drive](https://drive.google.com/drive/folders/1Rjb8dpyNnvvr0XLvIX9fg8Hdru_MhMCj?usp=sharing) / [Baidu (pwd:dcyb)](https://pan.baidu.com/s/1retfKIs_Om-D4zA45sL6Kg?pwd=dcyb) |

## Visual Results

**Figure 1.** perceptual comparison on outdoor scenes.

From top to bottom: input hazy image, official DEA-Net(HAZE4K 300 epochs) and our Jittor-DEA-Net(HAZE4K 30 epochs)

ä»ä¸Šå¾€ä¸‹åˆ†åˆ«ä¸ºè¾“å…¥ã€ä½¿ç”¨ä½œè€…é¢„è®­ç»ƒ300ä¸ª epoch çš„HAZE4Kæ•°æ®é›†æ¨¡å‹æ¨ç†ç»“æœï¼Œä»¥åŠä½¿ç”¨Jittoréƒ¨åˆ†è®­ç»ƒçš„æ¨¡å‹æ¨ç†ç»“æœï¼ˆä½¿ç”¨HAZE4Kæ•°æ®é›†è®­ç»ƒ30ä¸ªepochçš„æ¨¡å‹ï¼‰ã€‚
> ![Outdoor Dehazing Results Comparison](assets/outdoor.jpg)

**Figure 2.** perceptual comparison on indoor scenes.

From top to bottom: input hazy image, official DEA-Net(ITS 300 epochs), our Jittor-DEA-Net(ITS 10 epochs) and our Jittor-DEA-Net(ITS 100 epochs)

ä»ä¸Šå¾€ä¸‹åˆ†åˆ«ä¸ºè¾“å…¥ã€ä½¿ç”¨ä½œè€…é¢„è®­ç»ƒ300ä¸ª epoch çš„ITSæ•°æ®é›†æ¨¡å‹æ¨ç†ç»“æœï¼Œä»¥åŠä½¿ç”¨Jittoréƒ¨åˆ†è®­ç»ƒçš„æ¨¡å‹æ¨ç†ç»“æœï¼ˆåˆ†åˆ«è®­ç»ƒäº†10ä¸ªepochå’Œ100ä¸ªepochçš„ITSæ•°æ®é›†æ¨¡å‹ï¼‰ã€‚
> ![Indoor Dehazing Results Comparison](assets/indoor.jpg)

**Note:**  
The images shown here are real-world photographs with arbitrary resolutions and are not included in the training dataset. Due to the distribution gap between synthetic training data and real-world scenes, the dehazing performance may not be optimal in some cases. These results are provided for qualitative evaluation of the modelâ€™s dehazing capability and generalization ability in real-world scenarios.

The image results, from top to bottom, are the input, the inference results using the model pre-trained by the author for 300 epochs, and the inference results of part of the model trained with Jittor (Figure 1 uses the HAZE4K_30epochs model and Figure 2 uses the ITS_10epochs model and the ITS_100epochs model respectively). As shown in the figure, our trained model can definitely achieve the dehazing effect, but due to the limited number of training iterations and the use of a synthetic dataset, domain shift still causes artifacts that are visible to the naked eye. As the number of training epochs increases, the dehazing effect enhances and artifacts decrease accordingly (refer to Figure 2).

**æ³¨æ„ï¼š**  
æ­¤å¤„å±•ç¤ºçš„å›¾åƒå‡ä¸ºåœ¨çœŸå®åœºæ™¯ä¸‹é‡‡é›†çš„ã€å°ºå¯¸ä¸å—é™åˆ¶çš„è‡ªç„¶å›¾åƒï¼Œå‡æœªåŒ…å«åœ¨è®­ç»ƒæ•°æ®é›†ä¸­ã€‚ç”±äºå­˜åœ¨æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼Œæ¨¡å‹åœ¨éƒ¨åˆ†åŒºåŸŸçš„å»é›¾æ•ˆæœå¯èƒ½ä¸å¤Ÿç†æƒ³ã€‚æœ¬èŠ‚ç»“æœä¸»è¦ç”¨äºå®šæ€§å±•ç¤ºæ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„å»é›¾èƒ½åŠ›åŠå…¶æ³›åŒ–æ€§èƒ½ã€‚

å›¾ç‰‡ç»“æœä»ä¸Šå¾€ä¸‹åˆ†åˆ«ä¸ºè¾“å…¥ã€ä½¿ç”¨ä½œè€…é¢„è®­ç»ƒ300ä¸ª epoch çš„æ¨¡å‹æ¨ç†ç»“æœï¼Œä»¥åŠä½¿ç”¨ Jittor è®­ç»ƒçš„éƒ¨åˆ†æ¨¡å‹æ¨ç†ç»“æœ(å›¾ä¸€ä½¿ç”¨HAZE4K_30epochsæ¨¡å‹ï¼Œå›¾äºŒåˆ†åˆ«ä½¿ç”¨ITS_10epochsæ¨¡å‹ä¸ITS_100epochsæ¨¡å‹)ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å¯ä»¥èµ·åˆ°ä¸€å®šçš„å»é›¾æ•ˆæœï¼Œä½†ç”±äºè®­ç»ƒæ¬¡æ•°æœ‰é™ä¸”ä½¿ç”¨çš„æ˜¯åˆæˆæ•°æ®é›†ï¼ŒåŸŸåç§»ä»ä¼šå¯¼è‡´è‚‰çœ¼å¯è§çš„ä¼ªå½±ï¼Œéšç€è®­ç»ƒepochæ•°çš„å¢åŠ ï¼Œå»é›¾æ•ˆæœå¢å¼ºï¼Œä¼ªå½±ä¹Ÿéšä¹‹å‡å°‘ï¼ˆå¯å‚è€ƒå›¾äºŒï¼‰ã€‚

---

## âš™ï¸ Installation (å®‰è£…æŒ‡å—)

### Prerequisites
- Linux (Ubuntu 20.04+ recommended)
- Python 3.8+
- NVIDIA GPU + CUDA

### Setup
1. **Clone the repository:**
    ```bash
    git clone [https://github.com/ZR-1N/Jittor-DEA-Net.git](https://github.com/ZR-1N/Jittor-DEA-Net.git)
    cd Jittor-DEA-Net
    ```
2. **Create env:**
    ```bash
    conda create -n jittor_env python=3.8 -y
    conda activate jittor_env
    ```
3. **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Key dependencies: `jittor`, `numpy`, `Pillow`, `matplotlib`, `tqdm`.*

---

## ğŸ“‚ Data Preparation (æ•°æ®å‡†å¤‡)

Please download the datasets and organize them strictly as follows.

**Note**: Please download RESIDE-Standard version RESIDE dataset.

è¯·ä¸‹è½½æ•°æ®é›†å¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç›®å½•ç»“æ„æ•´ç†ï¼ˆä»£ç å°†è‡ªåŠ¨è¯†åˆ«è¯¥ç»“æ„ï¼‰ã€‚

**æ³¨æ„**ï¼šRESIDEæ•°æ®é›†è¯·ä¸‹è½½RESIDE-Standardç‰ˆæœ¬çš„ã€‚

**Download Links:** [RESIDE (ITS/OTS)](https://sites.google.com/view/reside-dehaze-datasets/reside-v0) | [HAZE4K](https://github.com/liuye123321/DMT-Net)

```text
Jittor-DEA-Net/
â”œâ”€â”€ code/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ HAZE4K/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”‚   â”œâ”€â”€ hazy/   (Contains .png/.jpg images)
â”‚   â”‚   â”‚   â””â”€â”€ clear/  (Contains .png/.jpg images)
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”‚       â”œâ”€â”€ hazy/
â”‚   â”‚       â””â”€â”€ clear/
â”‚   â”œâ”€â”€ ITS/
â”‚   â”‚   â”œâ”€â”€ train/ ... (Same structure as above)
â”‚   â”‚   â””â”€â”€ test/  ...
â”‚   â””â”€â”€ OTS/
â”‚   â”‚   â”œâ”€â”€ train/ ... (Same structure as above)
â”‚   â”‚   â””â”€â”€ test/  ...
â””â”€â”€ ...
```

---

## ğŸ”¥ Training (è®­ç»ƒ)

We provide training scripts for different datasets. The code automatically handles `.png` and `.jpg` matching.

æˆ‘ä»¬æä¾›äº†é’ˆå¯¹ä¸åŒæ•°æ®é›†çš„è®­ç»ƒè„šæœ¬ï¼Œä»£ç å·²è‡ªåŠ¨é€‚é… `.png` å’Œ `.jpg` çš„æ–‡ä»¶ååŒ¹é…ã€‚

### 1. Train on HAZE4K
```bash
cd code
CUDA_VISIBLE_DEVICES=0 nohup python train.py \
  --model_name DEA-Net-CR-HAZE4K \
  --dataset HAZE4K \
  --epochs 300 \
  --bs 4 \
  --w_loss_CR 0.1 \
  > training_haze4k.log 2>&1 &
```

### 2. Train on RESIDE-ITS (Indoor)
```bash
cd code
CUDA_VISIBLE_DEVICES=0 nohup python train.py \
  --model_name DEA-Net-CR-ITS \
  --dataset ITS \
  --epochs 300 \
  --bs 4 \
  --w_loss_CR 0.1 \
  > training_its.log 2>&1 &
```

### 3. Train on RESIDE-OTS (Outdoor)
```bash
cd code
CUDA_VISIBLE_DEVICES=0 nohup python train.py \
  --model_name DEA-Net-CR-OTS \
  --dataset OTS \
  --epochs 10 \
  --bs 4 \
  --w_loss_CR 0.1 \
  > training_ots.log 2>&1 &
```

*Training logs and checkpoints will be saved in `experiment/`.*

---

## ğŸ–¼ï¸ Inference (æ¨ç†)

This project offers two reasoning modes, corresponding respectively to the different states of the model before and after the **structural reparameterization** transformation. Both scripts have built-in automatic Padding logic and support image input of any resolution.

æœ¬é¡¹ç›®æä¾›ä¸¤ç§æ¨ç†æ¨¡å¼ï¼Œåˆ†åˆ«å¯¹åº”æ¨¡å‹åœ¨ **ç»“æ„é‡å‚æ•°åŒ–** è½¬æ¢å‰åçš„ä¸åŒçŠ¶æ€ã€‚
ä¸¤ç§è„šæœ¬å‡å†…ç½®è‡ªåŠ¨ Padding é€»è¾‘ï¼Œæ”¯æŒä»»æ„åˆ†è¾¨ç‡çš„å›¾åƒè¾“å…¥ã€‚

### 1. æ¨¡å¼å¯¹æ¯”ï¼šRaw vs Fused

| è„šæœ¬åç§° | é€‚ç”¨åœºæ™¯ | å¯¹åº”æ¨¡å‹æ¶æ„ | æ ¸å¿ƒé€»è¾‘ |
| :--- | :--- | :--- | :--- |
| **`inference_raw.py`** | éªŒè¯åˆšè®­ç»ƒå®Œçš„æ¨¡å‹ (æœªèåˆ) | `DEANet` (è®­ç»ƒç‰ˆæ¶æ„) | ä½¿ç”¨åŒ…å« 5 è·¯åˆ†æ”¯çš„ DEConv æ¨¡å—  |
| **`inference_fused.py`** | æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹æˆ–èåˆåçš„æ¨¡å‹ | `Backbone` (æ¨ç†ç‰ˆæ¶æ„) | ä½¿ç”¨æ•°å­¦èåˆåçš„å•è·¯æ™®é€šå·ç§¯  |

**ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªè„šæœ¬ï¼Ÿ**
- **è®­ç»ƒé˜¶æ®µ (Raw)**: 
To enhance the feature extraction capability, the model employs a parallel convolution that includes five branches such as central difference and angular difference. At this point, the weight file (such as' best.pk ') contains multiple parameters.
ä¸ºäº†å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œæ¨¡å‹ä½¿ç”¨äº†åŒ…å«ä¸­å¿ƒå·®åˆ†ã€è§’åº¦å·®åˆ†ç­‰ 5 ä¸ªåˆ†æ”¯çš„å¹¶è¡Œå·ç§¯ ã€‚æ­¤æ—¶çš„æƒé‡æ–‡ä»¶ï¼ˆå¦‚ `best.pk`ï¼‰åŒ…å«å¤šè·¯å‚æ•°ã€‚

- **æ¨ç†é˜¶æ®µ (Fused)**: 
By executing 'reparam.py', we fuse the 5-way weights into 1 way, thereby simplifying the complex 'DEConv' into the ordinary 'nn.Conv2d'.
é€šè¿‡æ‰§è¡Œ `reparam.py`ï¼Œæˆ‘ä»¬å°† 5 è·¯æƒé‡èåˆä¸º 1 è·¯ï¼Œä»è€Œå°†å¤æ‚çš„ `DEConv` ç®€åŒ–ä¸ºæ™®é€šçš„ `nn.Conv2d` ã€‚

- **åŒ¹é…è§„åˆ™(Rule)**: 
If you want to directly use your newly trained '.pk 'model, you must use' inference_raw.py '.
If you want to use the original author's 300-epoch pre-trained model ('.pth ') or your own reparameterized model, you must use 'inference_fused.py'
å¦‚æœä½ æƒ³ç›´æ¥ç”¨è‡ªå·±åˆšè®­ç»ƒå‡ºçš„ `.pk` æ¨¡å‹ï¼Œå¿…é¡»ä½¿ç”¨ `inference_raw.py` ;
å¦‚æœä½ æƒ³ä½¿ç”¨åŸä½œè€… 300 Epoch çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆ`.pth`ï¼‰æˆ–è‡ªå·±é‡å‚æ•°åŒ–åçš„æ¨¡å‹ï¼Œå¿…é¡»ä½¿ç”¨ `inference_fused.py` ã€‚
---

### 2. ä½¿ç”¨æ•™ç¨‹ (Usage)

#### ğŸš€ ä½¿ç”¨è‡ªè¡Œè®­ç»ƒçš„åŸå§‹æ¨¡å‹ (Raw Mode)
It is applicable to testing the initial training weights that have not yet run 'reparam.py'

é€‚ç”¨äºæµ‹è¯•å°šæœªè¿è¡Œ `reparam.py` çš„åˆå§‹è®­ç»ƒæƒé‡ï¼š
```bash
cd code
python3 inference_raw.py \
  --input_dir ../my_hazy_images \
  --output_dir ../my_results_raw \
  --model_path ../experiment/HAZE4K/DEA-Net-CR-HAZE4K/saved_model/best.pk
```

 #### âš¡ ä½¿ç”¨ä½œè€…é¢„è®­ç»ƒæˆ–å·²èåˆçš„æ¨¡å‹ (Fused Mode)
It is applicable to testing the weights provided by the original author or the 'best_fused.pk' you generated yourself

é€‚ç”¨äºæµ‹è¯•åŸä½œè€…æä¾›çš„æƒé‡ï¼Œæˆ–æ‚¨è‡ªè¡Œç”Ÿæˆçš„ `best_fused.pk`ï¼š
```bash
cd code
python3 inference_fused.py \
  --input_dir ../my_hazy_images \
  --output_dir ../my_results_fused \
  --model_path ../trained_models/ITS/PSNR4131_SSIM9945.pth
```

### 3.å»ºè®® (Recommendation)

A more recommended approach is to re-parameterize the model through reparam.py and then use inference_fused.py for inference. The reason is that after re-parameterization, the five parallel convolution re-parameters are combined into one vanilla convolution, which can significantly accelerate the inference efficiency.

æ›´æ¨èçš„æ–¹å¼æ˜¯é€šè¿‡reparam.pyå°†æ¨¡å‹è¿›è¡Œé‡å‚æ•°åŒ–åä½¿ç”¨inference_fused.pyè¿›è¡Œæ¨ç†ï¼ŒåŸå› åœ¨äºé‡å‚æ•°åŒ–åäº”ä¸ªå¹¶è¡Œå·ç§¯é‡å‚æ•°ä¸ºä¸€ä¸ªæ™®é€šå·ç§¯ï¼Œæ›´å¤Ÿæ˜¾è‘—åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚

---

## ğŸ”— Acknowledgements & Citation (è‡´è°¢ä¸å¼•ç”¨)

This project is based on the official PyTorch implementation of [DEA-Net](https://github.com/cecret3350/DEA-Net). We thank the authors for their excellent work.

If you find this repository useful, please consider citing the original paper:

```bibtex
@article{chen2023dea,
  title={DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention},
  author={Chen, Zixuan and He, Zewei and Lu, Zhe-Ming},
  journal={IEEE Transactions on Image Processing},
  year={2024},
  volume={33},
  pages={1002-1015}
}
```

## ğŸ“§ Contact

For any questions regarding this Jittor implementation, please contact:
**Shang Wenxuan (å°šæ–‡è½©)**: shangwenxuan.nku@gmail.com