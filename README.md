<div align="center">

<img src="assets/Nankai_University_logo.svg" height="80px" alt="Nankai University" >
<img src="assets/JittorLogo_Final1220.svg" height="80px" alt="Jittor" >

# Jittor-DEA-Net

**DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention (IEEE TIP 2024)**

[![Jittor](https://img.shields.io/badge/Framework-Jittor-EA3323.svg)](https://cg.cs.tsinghua.edu.cn/jittor/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/ZR-1N/Jittor-DEA-Net?style=social)](https://github.com/ZR-1N/Jittor-DEA-Net)

[![Paper](https://img.shields.io/badge/arXiv-Paper-b31b1b.svg)](https://arxiv.org/abs/2301.04805)
[![Original Repo](https://img.shields.io/badge/Official-PyTorch_Repo-EE4C2C.svg)](https://github.com/cecret3350/DEA-Net)

[English](#-introduction) | [ç®€ä½“ä¸­æ–‡](#-é¡¹ç›®ç®€ä»‹)

</div>

---

## ğŸ“– Introduction

This repository is an official implementation of **DEA-Net** based on the [Jittor (è®¡å›¾)](https://cg.cs.tsinghua.edu.cn/jittor/) deep learning framework. This project is part of the **"Sprouts Program" at Nankai University**.

DEA-Net proposes a novel detail-enhanced convolution (DEConv) and content-guided attention (CGA) mechanism to effectively restore haze-free images. By leveraging Jittor's **Just-In-Time (JIT) compilation** and **operator fusion**, this implementation achieves competitive training efficiency compared to the PyTorch version while maintaining algorithmic performance.

### Overall Architecture
<div align="center">
  <img src="fig/architecture.png" alt="Overall Architecture" width="90%">
</div>

### Results

<img src="fig/results.png" alt="Results" style="zoom:20%;" />

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ IEEE TIP 2024 è®ºæ–‡ **DEA-Net** çš„ **Jittor (è®¡å›¾)** ç‰ˆæœ¬å¤ç°ï¼Œå±äº **å—å¼€å¤§å­¦â€œæ–°èŠ½è®¡åˆ’â€** å­¦ä¹ æˆæœã€‚

DEA-Net æå‡ºäº†ä¸€ç§ç»†èŠ‚å¢å¼ºå·ç§¯ï¼ˆDEConvï¼‰å’Œå†…å®¹å¼•å¯¼æ³¨æ„åŠ›ï¼ˆCGAï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤å»é›¾å›¾åƒã€‚å¾—ç›Šäº Jittor æ¡†æ¶çš„ **å³æ—¶ç¼–è¯‘ (JIT)** å’Œ **ç®—å­èåˆ** æŠ€æœ¯ï¼Œæœ¬é¡¹ç›®åœ¨ä¿æŒåŸè®ºæ–‡ç²¾åº¦çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒä¸æ¨ç†ã€‚

---

## ğŸ“° News

- **[2025-12-21]** ğŸš€ Initial release of Jittor-DEA-Net code and pre-trained weights for HAZE4K, ITS, and OTS datasets.
- **[2025-11-17]** ğŸ—ï¸ Project initialized under Nankai University "Sprouts Program".

---

## ğŸ“Š Model Zoo & Results (æ¨¡å‹åº“ä¸ç»“æœå¯¹æ¯”)

We provide a comparison between our Jittor implementation (Partial Training) and the official PyTorch implementation (Full Converged Training).

**Note:** The Jittor weights provided below are from the initial training phase (e.g., 10-30 epochs), yet they already demonstrate strong performance. The official PyTorch models were trained for 300 epochs.

**æ³¨æ„ï¼š** ä¸‹æ–¹æä¾›çš„ Jittor æƒé‡å¤„äºè®­ç»ƒåˆæœŸé˜¶æ®µï¼ˆä»… 10-30 Epochï¼‰ï¼Œä½†å·²å±•ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚å®˜æ–¹ PyTorch æ¨¡å‹ä¸ºå®Œæ•´è®­ç»ƒ 300 Epoch çš„ç»“æœã€‚

| Dataset | Framework | Epochs Trained | PSNR (dB) | SSIM | Download Link |
| :--- | :--- | :---: | :---: | :---: | :---: |
| **HAZE4K** | **Jittor (Ours)** | **30** (Partial) | 32.54 | 0.9848 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | PyTorch (Official) | 300 | 34.26 | 0.9985 | [Google Drive](https://drive.google.com/drive/folders/1Rjb8dpyNnvvr0XLvIX9fg8Hdru_MhMCj?usp=sharing) / [Baidu (pwd:dcyb)](https://pan.baidu.com/s/1retfKIs_Om-D4zA45sL6Kg?pwd=dcyb) |
| **RESIDE-ITS** | **Jittor (Ours)** | **10** (Partial) | 35.87 | 0.9893 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | PyTorch (Official) | 300 | 41.31 | 0.9945 | [Google Drive](https://drive.google.com/drive/folders/1Rjb8dpyNnvvr0XLvIX9fg8Hdru_MhMCj?usp=sharing) / [Baidu (pwd:dcyb)](https://pan.baidu.com/s/1retfKIs_Om-D4zA45sL6Kg?pwd=dcyb) |
| **RESIDE-OTS** | **Jittor (Ours)** | **10** (Partial) | 32.71 | 0.9840 | [Google Drive (Ours)](https://drive.google.com/drive/folders/1MN1alc4gBzk90Vc8V1AXivwx2FwrF5f3?usp=sharing) |
| | PyTorch (Official) | 300 | 36.59 | 0.9897 | [Google Drive](https://drive.google.com/drive/folders/1Rjb8dpyNnvvr0XLvIX9fg8Hdru_MhMCj?usp=sharing) / [Baidu (pwd:dcyb)](https://pan.baidu.com/s/1retfKIs_Om-D4zA45sL6Kg?pwd=dcyb) |

## Visual Results
> ![Outdoor Dehazing Results Comparison](assets/outdoor.jpg)
> ![Indoor Dehazing Results Comparison](assets/indoor.jpg)


**Note:**  
The image results, from top to bottom, represent the input, the inference result using the model pre-trained by the authors for 300 epochs, and the inference result using a partially trained model trained by Jittor. As shown in the figure, our trained model can definitely achieve the dehazing effect, but due to the limited number of training iterations and the use of a synthetic dataset, domain offset still causes artifacts that are visible to the naked eye.

**æ³¨æ„ï¼š**  
å›¾ç‰‡ç»“æœä»ä¸Šå¾€ä¸‹åˆ†åˆ«ä¸ºè¾“å…¥ã€ä½¿ç”¨ä½œè€…é¢„è®­ç»ƒ300ä¸ª epoch çš„æ¨¡å‹æ¨ç†ç»“æœï¼Œä»¥åŠä½¿ç”¨ Jittor è®­ç»ƒçš„éƒ¨åˆ†æ¨¡å‹æ¨ç†ç»“æœã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å¯ä»¥èµ·åˆ°ä¸€å®šçš„å»é›¾æ•ˆæœï¼Œä½†ç”±äºè®­ç»ƒæ¬¡æ•°æœ‰é™ä¸”ä½¿ç”¨çš„æ˜¯åˆæˆæ•°æ®é›†ï¼ŒåŸŸåç§»ä»ä¼šå¯¼è‡´è‚‰çœ¼å¯è§çš„ä¼ªå½±ã€‚

---

## âš™ï¸ Installation (å®‰è£…æŒ‡å—)

### Prerequisites
- Linux (Ubuntu 20.04+ recommended)
- Python 3.8+
- NVIDIA GPU + CUDA

### Setup
1. **Clone the repository:**
    ```bash
    git clone [https://github.com/ZR-1N/Jittor-DEA-Net.git](https://github.com/ZR-1N/Jittor-DEA-Net.git)
    cd Jittor-DEA-Net
    ```

2. **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Key dependencies: `jittor`, `numpy`, `Pillow`, `matplotlib`, `tqdm`.*

---

## ğŸ“‚ Data Preparation (æ•°æ®å‡†å¤‡)

Please download the datasets and organize them strictly as follows.
è¯·ä¸‹è½½æ•°æ®é›†å¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç›®å½•ç»“æ„æ•´ç†ï¼ˆä»£ç å°†è‡ªåŠ¨è¯†åˆ«è¯¥ç»“æ„ï¼‰ã€‚

**Download Links:** [RESIDE (ITS/OTS)](https://sites.google.com/view/reside-dehaze-datasets/reside-v0) | [HAZE4K](https://github.com/liuye123321/DMT-Net)

```text
Jittor-DEA-Net/
â”œâ”€â”€ code/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ HAZE4K/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”‚   â”œâ”€â”€ hazy/   (Contains .png/.jpg images)
â”‚   â”‚   â”‚   â””â”€â”€ clear/  (Contains .png/.jpg images)
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”‚       â”œâ”€â”€ hazy/
â”‚   â”‚       â””â”€â”€ clear/
â”‚   â”œâ”€â”€ ITS/
â”‚   â”‚   â”œâ”€â”€ train/ ... (Same structure as above)
â”‚   â”‚   â””â”€â”€ test/  ...
â”‚   â””â”€â”€ OTS/
â”‚   â”‚   â”œâ”€â”€ train/ ... (Same structure as above)
â”‚   â”‚   â””â”€â”€ test/  ...
â””â”€â”€ ...
```

---

## ğŸ”¥ Training (è®­ç»ƒ)

We provide training scripts for different datasets. The code automatically handles `.png` and `.jpg` matching.
æˆ‘ä»¬æä¾›äº†é’ˆå¯¹ä¸åŒæ•°æ®é›†çš„è®­ç»ƒè„šæœ¬ï¼Œä»£ç å·²è‡ªåŠ¨é€‚é… `.png` å’Œ `.jpg` çš„æ–‡ä»¶ååŒ¹é…ã€‚

### 1. Train on HAZE4K
```bash
cd code
CUDA_VISIBLE_DEVICES=0 nohup python train.py \
  --model_name DEA-Net-CR-HAZE4K \
  --dataset HAZE4K \
  --epochs 300 \
  --bs 4 \
  --w_loss_CR 0.1 \
  > training_haze4k.log 2>&1 &
```

### 2. Train on RESIDE-ITS (Indoor)
```bash
cd code
CUDA_VISIBLE_DEVICES=0 nohup python train.py \
  --model_name DEA-Net-CR-ITS \
  --dataset ITS \
  --epochs 300 \
  --bs 4 \
  --w_loss_CR 0.1 \
  > training_its.log 2>&1 &
```

### 3. Train on RESIDE-OTS (Outdoor)
```bash
cd code
CUDA_VISIBLE_DEVICES=0 nohup python train.py \
  --model_name DEA-Net-CR-OTS \
  --dataset OTS \
  --epochs 10 \
  --bs 4 \
  --w_loss_CR 0.1 \
  > training_ots.log 2>&1 &
```

*Training logs and checkpoints will be saved in `experiment/`.*

---

## ğŸ–¼ï¸ Inference (æ¨ç†)

Use `inference_raw.py` to dehaze your own images. The script automatically pads images to support arbitrary resolutions.
ä½¿ç”¨ `inference_raw.py` å¯¹è‡ªå®šä¹‰å›¾åƒè¿›è¡Œå»é›¾ã€‚è„šæœ¬ä¼šè‡ªåŠ¨å¯¹å›¾åƒè¿›è¡Œ Padding ä»¥æ”¯æŒä»»æ„åˆ†è¾¨ç‡ã€‚

```bash
cd code
python3 inference_raw.py \
  --input_dir ../my_hazy_images \
  --output_dir ../my_results \
  --model_path ../experiment/HAZE4K/DEA-Net-CR-HAZE4K/saved_model/PSNR3254__SSIM9848.pk
```

---

## ğŸ”— Acknowledgements & Citation (è‡´è°¢ä¸å¼•ç”¨)

This project is based on the official PyTorch implementation of [DEA-Net](https://github.com/cecret3350/DEA-Net). We thank the authors for their excellent work.

If you find this repository useful, please consider citing the original paper:

```bibtex
@article{chen2023dea,
  title={DEA-Net: Single image dehazing based on detail-enhanced convolution and content-guided attention},
  author={Chen, Zixuan and He, Zewei and Lu, Zhe-Ming},
  journal={IEEE Transactions on Image Processing},
  year={2024},
  volume={33},
  pages={1002-1015}
}
```

## ğŸ“§ Contact

For any questions regarding this Jittor implementation, please contact:
**Shang Wenxuan (å°šæ–‡è½©)**: shangwenxuan.nku@gmail.com